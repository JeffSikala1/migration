
# Needs to RUN after karpenter install via helm shell script
################################################################################
# Controller & Node IAM roles, SQS Queue, Eventbridge Rules
################################################################################
resource "terraform_data" "karpenterinstall" {
  provisioner "local-exec" {
    command = "./karpenterinstall.sh ${var.karpenterchartversion} ${module.eks.cluster_name} ${var.awsaccountid} ${module.eks.cluster_endpoint} ${module.eks.oidc_provider_arn}"
  }
  depends_on = [
    module.eks
  ]
}

/*output "karpenter_install_args" {
  description = "Check the params being passed to karpenterinstall.sh script"
  value       = "Karpenter install args - ${terraform_data.karpenterinstall.output}"
}*/

module "eks_karpenter" {
  source  = "terraform-aws-modules/eks/aws//modules/karpenter"
  version = ">= 20.24.2"
  #version = "~> 20.11" # pick whichever was supplied as submodule of eks

  cluster_name = module.eks.cluster_name
  iam_policy_name = "KarpenterControllerPolicy"
  iam_policy_use_name_prefix = false
  enable_v1_permissions = true
  create_instance_profile = true
  enable_pod_identity = true
  create_pod_identity_association = true
  # For IRSA
  #enable_irsa = true
  #node_iam_role_name              = module.eks.eks_managed_node_groups["karpenter"].iam_role_name
  #node_iam_role_use_name_prefix   = false
  #irsa_oidc_provider_arn          = "arn:aws:iam::${var.awsaccountid}:oidc-provider/oidc.eks.us-east-1.amazonaws.com"
  ####

  # Used to attach additional IAM policies to the Karpenter node IAM role
  node_iam_role_additional_policies = {
    AmazonSSMManagedInstanceCore = "arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore"
    AmazonEC2FullAccess          = "arn:aws:iam::aws:policy/AmazonEC2FullAccess"
  }

  # Name needs to match role name passed to the EC2NodeClass
  #node_iam_role_use_name_prefix   = false
  #node_iam_role_name              = var.eksclustername
  #create_pod_identity_association = true
  create_node_iam_role = false
  node_iam_role_arn    = module.eks.eks_managed_node_groups["karpenter"].iam_role_arn
  # Since the node group role will already have an access entry
  create_access_entry = false 

  tags = var.tags
  depends_on = [
    terraform_data.karpenterinstall
  ]
  
}

output "ekskarpenterrole" {
  description = "eks_karpenter node role name"
  value       = "${module.eks_karpenter.node_iam_role_arn}"
}
################################################################################
# Helm charts
################################################################################
#Don't use helm_release helm below <3 is broken with "public" ecr auth.
# terraform helm implementation via helm_release is very old.
#Personal opinion- helm charts are overly templated, thus when a new feature has
#to be implemented then not only have to understand the AWS aspect also the
#nuances of those N number of template variables to kubernetes manifest vars. So
#it is overkill for a handful of manifests. Will consider when there are > 50 manifests.
/*
resource "helm_release" "karpenter" {
  namespace           = "kube-system"
  name                = "karpenter"
  #repository          = "https://charts.karpenter.sh"
  repository          = "oci://public.ecr.aws/karpenter"
  //buggy fails login, most frustrating ecr #~$!  "oci://public.ecr.aws/karpenter"
  //repository_username = data.aws_ecrpublic_authorization_token.token.user_name
  //repository_password = data.aws_ecrpublic_authorization_token.token.password
  chart               = "karpenter"
  version             = var.karpenterchartversion
  wait                = false


  values = [
    <<-EOT
    serviceAccount:
      name: ${module.eks_karpenter.service_account}
    settings:
      clusterName: ${module.eks.cluster_name}
      clusterEndpoint: ${module.eks.cluster_endpoint}
      interruptionQueue: ${module.eks_karpenter.queue_name}
    EOT
  ]

  lifecycle {
    ignore_changes = [
      repository_password
    ]
  }

  //lifecycle {
  //  ignore_changes = all
  //} 

} //End resource helm_release.karpenter

*/
output "karpenterserviceaccount" {
  description = "Service account for karpenter"
  value = "${module.eks_karpenter.service_account} - ${module.eks.cluster_endpoint}"
}

/*resource "kubectl_manifest" "karpenter_node_class" {
  yaml_body = <<-YAML
    apiVersion: karpenter.k8s.aws/v1beta1
    kind: EC2NodeClass
    metadata:
      name: default
    spec:
      amiFamily: AL2
      role: ${module.eks.eks_managed_node_groups["karpenter"].iam_role_name}
      subnetSelectorTerms:
        - tags:
            Name: "Private-Services-az*"
      securityGroupSelectorTerms:
        - tags:
            karpenter.sh/discovery: ${module.eks.cluster_name}
      tags:
        karpenter.sh/discovery: ${module.eks.cluster_name}
      amiSelectorTerms:
        - id: ${var.eksamiid}

  YAML

  depends_on = [
    helm_release.karpenter
  ] 
}



resource "kubectl_manifest" "karpenter_node_pool" {
  yaml_body = <<-YAML
    apiVersion: karpenter.sh/v1beta1
    kind: NodePool
    metadata:
      name: default
    spec:
      template:
        spec:
          nodeClassRef:
            name: default
          requirements:
            - key: "karpenter.k8s.aws/instance-category"
              operator: In
              values: ["t", "c", "m", "r"]
            - key: "karpenter.k8s.aws/instance-cpu"
              operator: In
              values: ["4", "8", "16", "32"]
            - key: "karpenter.k8s.aws/instance-hypervisor"
              operator: In
              values: ["nitro"]
            - key: "karpenter.k8s.aws/instance-generation"
              operator: Gt
              values: ["2"]
            - key: "kubernetes.io/arch"
              operator: In
              values: ["amd64"]
            - key: "kubernetes.io/os"
              operator: In
              values: ["linux"]
            - key: karpenter.sh/capacity-type
              operator: In
              values: ["on-demand"]
      limits:
        cpu: 1000
      disruption:
        consolidationPolicy: WhenEmpty
        consolidateAfter: 30s
  YAML

  depends_on = [
    kubectl_manifest.karpenter_node_class
  ]
}
*/
